{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a pre-trained Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/.mxnet/models/alexnet-44335d1f.zip40d91a9a-8e3f-45dd-8d38-d6bff4243277 from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/alexnet-44335d1f.zip...\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon.model_zoo import vision\n",
    "import mxnet as mx\n",
    "\n",
    "num_outputs = 5\n",
    "\n",
    "# change for cpu or gpu\n",
    "ctx = mx.cpu()\n",
    "#ctx = mx.gpu()\n",
    "\n",
    "# Let's get the pre-trained network and copy weights\n",
    "pre_trained_net = vision.alexnet(pretrained=True, ctx=ctx)\n",
    "net = vision.alexnet(classes=num_outputs, ctx=ctx)\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "net.features = pre_trained_net.features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code piece prints the weights for each layer.It also shows the layer name and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter alexnet0_conv0_weight (shape=(64, 3, 11, 11), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv0_bias (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv1_weight (shape=(192, 64, 5, 5), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv1_bias (shape=(192,), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv2_weight (shape=(384, 192, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv2_bias (shape=(384,), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv3_weight (shape=(256, 384, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv3_bias (shape=(256,), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv4_weight (shape=(256, 256, 3, 3), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_conv4_bias (shape=(256,), dtype=<class 'numpy.float32'>)\n",
      "Parameter alexnet0_dense0_weight (shape=(4096, 9216), dtype=float32)\n",
      "Parameter alexnet0_dense0_bias (shape=(4096,), dtype=float32)\n",
      "Parameter alexnet0_dense1_weight (shape=(4096, 4096), dtype=float32)\n",
      "Parameter alexnet0_dense1_bias (shape=(4096,), dtype=float32)\n",
      "Parameter alexnet1_dense2_weight (shape=(5, 0), dtype=float32)\n",
      "Parameter alexnet1_dense2_bias (shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for param in net.collect_params().values():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's freeze all the layers until the flly connected layers (\"Dense layers\"). We can simply search for \"conv\" in the name of layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can freeze some layers using the code below \n",
    "for param in net.collect_params().values(): # Or some other layers that you want. \n",
    "    if \"conv\" in param.name:\n",
    "        param.grad_req = 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter alexnet0_conv0_weight (shape=(64, 3, 11, 11), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv0_bias (shape=(64,), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv1_weight (shape=(192, 64, 5, 5), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv1_bias (shape=(192,), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv2_weight (shape=(384, 192, 3, 3), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv2_bias (shape=(384,), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv3_weight (shape=(256, 384, 3, 3), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv3_bias (shape=(256,), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv4_weight (shape=(256, 256, 3, 3), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_conv4_bias (shape=(256,), dtype=<class 'numpy.float32'>) null\n",
      "Parameter alexnet0_dense0_weight (shape=(4096, 9216), dtype=float32) write\n",
      "Parameter alexnet0_dense0_bias (shape=(4096,), dtype=float32) write\n",
      "Parameter alexnet0_dense1_weight (shape=(4096, 4096), dtype=float32) write\n",
      "Parameter alexnet0_dense1_bias (shape=(4096,), dtype=float32) write\n",
      "Parameter alexnet1_dense2_weight (shape=(5, 0), dtype=float32) write\n",
      "Parameter alexnet1_dense2_bias (shape=(5,), dtype=float32) write\n"
     ]
    }
   ],
   "source": [
    "for param in net.collect_params().values():\n",
    "    print(param, param.grad_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
