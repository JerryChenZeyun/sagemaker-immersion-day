{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLA Logo](https://drive.corp.amazon.com/view/mrruckma@/MLA_headerv2.png?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may need to update the scikit-image module due to an [issue](https://github.com/numpy/numpy/issues/12744) that was resolved in later versions. Uncomment the below if you run into errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and Read the Dataset\n",
    "\n",
    "#### Our dataset is made of some Amazon product images. We have 5 categories: \"Ring\", \"Shirt\", \"Watch\", \"Jeans\" and \"Shoe\".\n",
    "![Images](https://drive.corp.amazon.com/view/MLA%20Team%20Drive/MLA-ALL-CLASSES-PROJECTS/figures/product_images.png?download=true)\n",
    "\n",
    "#### We have the following classes:\n",
    "### * Class 0: Ring\n",
    "### * Class 1: Shirt\n",
    "### * Class 2: Watch\n",
    "### * Class 3: Jeans\n",
    "### * Class 4: Shoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reading it into a Data Frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve (\n",
    "    'https://d8mrh1kj01ho9.cloudfront.net/workshop/cv1/data/day1/example_dataset.pkl', \n",
    "    \"/tmp/example_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"/tmp/example_dataset.pkl\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = ['Ring', 'Shirt', 'Watch', 'Jeans', 'Shoe']\n",
    "\n",
    "plt.imshow(df[\"data\"][551])\n",
    "plt.title(classes[df[\"class\"][551]])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(df[\"data\"][1119])\n",
    "plt.title(classes[df[\"class\"][1119]])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(df[\"data\"][156])\n",
    "plt.title(classes[df[\"class\"][156]])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(df[\"data\"][1526])\n",
    "plt.title(classes[df[\"class\"][1526]])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(df[\"data\"][15])\n",
    "plt.title(classes[df[\"class\"][15]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the dataset and Process Images\n",
    "#### * We split the original dataset into Training (80%), Validation (10%) and Test (10%) subsets. \n",
    "#### * We will process images by resizing to 224x224 and converting (row, column, channel) to (channel, row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from mxnet import gluon\n",
    "\n",
    "# Let's split to data into training (80%), validation (10%) and test (10%) subsets.\n",
    "train_indices = np.arange(0, int(0.8*len(df)))\n",
    "val_indices = np.arange(int(0.8*len(df)), int(0.9*len(df)))\n",
    "test_indices = np.arange(int(0.9*len(df)), len(df))\n",
    "\n",
    "train_df = df.iloc[train_indices]\n",
    "val_df = df.iloc[val_indices]\n",
    "test_df = df.iloc[test_indices]\n",
    "\n",
    "del df\n",
    "\n",
    "def getImages(images):\n",
    "    # Create the image holder array\n",
    "    image_arr = np.zeros((images.shape[0], 3, 224, 224), dtype=\"float32\")\n",
    "    \n",
    "    # Iterate through the image data\n",
    "    for i, im in enumerate(images):\n",
    "        # Get image from the data column of the current row\n",
    "        \n",
    "        # We need a fixed size input, our images have different sizes, let's pick 224x224.\n",
    "        # Resize image below\n",
    "        im = resize(im, output_shape=(224, 224))\n",
    "        \n",
    "        # Gluon/mxnet expects images in this format (channel, row, column)\n",
    "        # This is the opposite of (row, column, channel), let's fix it\n",
    "        im = np.moveaxis(im, -1, 0)\n",
    "        \n",
    "        # Assign the value in the image array\n",
    "        image_arr[i] = im\n",
    "\n",
    "    return image_arr\n",
    "    \n",
    "train_images, train_labels = getImages(train_df[\"data\"].values), train_df[\"class\"].values\n",
    "validation_images, validation_labels = getImages(val_df[\"data\"].values), val_df[\"class\"].values\n",
    "\n",
    "# Using Gluon Data loaders to load the data in batches\n",
    "train_dataset = gluon.data.ArrayDataset(train_images, train_labels)\n",
    "validation_dataset = gluon.data.ArrayDataset(validation_images, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "plt.figure()\n",
    "ax = sb.countplot(x=train_labels, y = None)\n",
    "for p in ax.patches:\n",
    "    x=p.get_bbox().get_points()[:,0]\n",
    "    y=p.get_bbox().get_points()[1,1]\n",
    "    ax.annotate(y, (x.mean(), y), ha='center', va='bottom') \n",
    "plt.xticks(rotation=60)\n",
    "plt.title('Number of records in each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Confusion Matrix \n",
    "#### We have the following function to plot confusion matrix. We will call this function to plot confusion matrix for our validation data after each epoch (one full pass through the training dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in ['ring', 'shirt', 'watch', 'jeans', 'shoe']], columns = [i for i in ['ring', 'shirt', 'watch', 'jeans', 'shoe']])\n",
    "    plt.figure(figsize = (5, 5))\n",
    "    plt.title('Validation Confusion Matrix')\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training and Validation\n",
    "#### We will do the following:\n",
    "#### * Create a simple Convolutional Neural Network (2 conv-pool pairs + flattening + softmax)\n",
    "#### * Use the following hyper-parameters: batch size, epochs, learning rate. You can experiment with different values, but we will discuss those in greater detail tomorrow\n",
    "#### * Calculate the loss and accuracy for training and validation at each epoch and print them at the end of each epoch.\n",
    "#### * Plot the confusion matrix at the end of each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, ndarray\n",
    "from mxnet.gluon.loss import SoftmaxCrossEntropyLoss\n",
    "import mxnet.ndarray as nd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set this to CPU or GPU depending on your training instance\n",
    "# ctx = mx.cpu()\n",
    "ctx = mx.gpu()\n",
    "\n",
    "# Hyper-paramaters of the system\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create the network. We have 5 classes\n",
    "num_outputs = 5\n",
    "    \n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    net.add(gluon.nn.Conv2D(channels=60, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(512, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))\n",
    "    \n",
    "# Initialize parameters\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "# Define loss and trainer.\n",
    "softmax_cross_etropy_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate})\n",
    "\n",
    "train_loader = gluon.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "validation_loader = gluon.data.DataLoader(validation_dataset, batch_size=batch_size)\n",
    "\n",
    "# Starting the outer loop, we will have 3 epochs (3 full pass through our dataset)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training loop: (with autograd and trainer steps, etc.)\n",
    "    # This loop does the training of the neural network (weights are updated)\n",
    "    cumulative_train_loss = 0\n",
    "    train_predictions = []\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            train_predictions = train_predictions + np.argmax(output.asnumpy(), axis=1).tolist()\n",
    "            loss = softmax_cross_etropy_loss(output, label)\n",
    "            cumulative_train_loss = cumulative_train_loss + nd.sum(loss)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    \n",
    "    # Calculating the Softmax Cross Entopy Loss for training\n",
    "    train_loss = cumulative_train_loss/len(train_images)\n",
    "\n",
    "    # Validation loop:\n",
    "    # This loop tests the trained network on validation dataset\n",
    "    # No weight updates here\n",
    "    cumulative_valid_loss = 0\n",
    "    val_predictions = []\n",
    "    for i, (data, label) in enumerate(validation_loader):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        val_predictions = val_predictions + np.argmax(output.asnumpy(), axis=1).tolist()\n",
    "        val_loss = softmax_cross_etropy_loss(output, label)\n",
    "        cumulative_valid_loss = cumulative_valid_loss + nd.sum(val_loss)\n",
    "    valid_loss = cumulative_valid_loss/len(validation_images)\n",
    "\n",
    "    # Calculate training and validation accuracies\n",
    "    # I used a accuracy_score() function from the sklearn library here. \n",
    "    # accuracy = (TP+TN) / (TP+FP+TN+FN)\n",
    "    train_accuracy = accuracy_score(train_labels.tolist(), train_predictions)\n",
    "    validation_accuracy = accuracy_score(validation_labels.tolist(), val_predictions)\n",
    "    \n",
    "    # Print the summary and plot the confusion matrix after each epoch\n",
    "    print(\"Epoch {}, training loss: {:.2f}, validation loss: {:.2f}, training accuracy: {:.2f}, validation accuracy: {:.2f}\".format(epoch, train_loss.asnumpy()[0], valid_loss.asnumpy()[0], train_accuracy, validation_accuracy))\n",
    "    plot_confusion_matrix(validation_labels.tolist(), val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test on test dataset\n",
    "#### We are done with the training and validation. Let's get the test predictions below. We will also write them to a __CSV__ file in a similar way to our __Final Project__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = getImages(test_df[\"data\"].values)\n",
    "test_loader = gluon.data.DataLoader(test_images, batch_size=batch_size)\n",
    "\n",
    "test_predictions = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    data = data.as_in_context(ctx)\n",
    "    output = net(data)\n",
    "    test_predictions = test_predictions + np.argmax(output.asnumpy(), axis=1).tolist()\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame(columns=['ID', 'class'])\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"class\"] = test_predictions\n",
    "\n",
    "result_df.to_csv(\"/tmp/results_cv_example.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
